#summary A selection of useful resources for various Programming Languages.

<wiki:toc max_depth="3" />

= Java =

[http://www.oracle.com/technetwork/java/index-141834.html Oracle's FAQ for all Java topics].

== P.I.E. - The Pillars of Object Orientation ==

=== Polymorphism ===

[http://en.wikipedia.org/wiki/Polymorphism_in_object-oriented_programming Wikipedia: Polymorphism]

  * Is the ability of methods to behave differently, based on the object calling it

=== Inheritance ===

[http://en.wikipedia.org/wiki/Inheritance_(computer_science) Wikipedia: Inheritance]

  * In simple words, Inheritance is way to define new a class, using classes which have already been defined

Inheritance [is a] vs. Composition [has a]

*Inheritance* is uni-directional. For example _House_ *is a* _Building_, but _Building_ is not a _House_. *Inheritance* uses the *extends* keyword. *Composition* on the other hand is used when _House_ *has a* _Bathroom_. Inheritance should not be used just to facilitate code reuse, if there is no *is a* relationship then use composition for code reuse. 

=== Encapsulation ===

[http://en.wikipedia.org/wiki/Encapsulation_(object-oriented_programming) Wikipedia: Encapsulation]

  * With Encapsulation you can hide (restrict access) to critical data members in your code , which improves security
  * Encapsualtion combines data and actions together (just like a capsule)

==== Data Abstraction ====

[http://wiki.answers.com/Q/What_is_data_abstraction_in_java Data Abstraction in Java]

  * Abstraction in the process of selecting important data sets for an Object in your software , and leaving out the insignificant ones.
  * Once you have modeled your object using Abstraction , the same set of data could be used in different applications.

== Autoboxing ==

Autoboxing is available as of Java 1.5.0 and introduces the automated handling of primitive Java type to their equivalent Wrapper class. [http://java.sun.com/j2se/1.5.0/docs/guide/language/autoboxing.html Java 1.5.0 Language Guilde]

== Imports & Performace ==

Did some research and it seems that conventional wisdom suggests that using imports with the wild card character, of the form:
{{{
import javax.swing.*;
}}}
does not impact runtime performance. There is however a valid case for specifying exactly the classes you wish to import as detailed in [http://www.javafaq.nu/java-article914.html this article].

== Mutable & Immutable Objects ==

[http://www.javaranch.com/journal/2003/04/immutable.htm Java Ranch - Mutable and Immutable Objects]

== Generics ==

[http://onjava.com/onjava/excerpt/javaian5_chap04/index1.html Writing Generic Types & Methods]

== Checked & Unchecked Exceptions ==

[http://tutorials.jenkov.com/java-exception-handling/checked-or-unchecked-exceptions.html Java Exception Handling - Checked or Unchecked Exceptions]

== Challenging Java Questions ==

[http://robaustin.wikidot.com/50-java-interview-questions Java Interview Questions]

== Threading & Lock Contention ==

[http://www.thinkingparallel.com/2007/07/31/10-ways-to-reduce-lock-contention-in-threaded-programs/ 10 Ways to Reduce Lock Contention in Threaded Programs]

----

= Java EE =

[http://docs.oracle.com/javaee/6/tutorial/doc/docinfo.html Jave EE 6 Tutorial]

----

= XML =

== SAX & DOM ==

SAX - Event Driven, stack based parsing, akin to shift-reduce mechanism employed by bottom-up parsers. [http://en.wikipedia.org/wiki/Simple_API_for_XML Wikipedia]

DOM - Tree Walking [http://en.wikipedia.org/wiki/Document_Object_Model Wikipedia]

[http://onjava.com/onjava/2002/06/26/xml.html SAX & DOM, the Basics]

[http://www.informit.com/library/library.aspx?b=STY_XML_21days Teach Yourself XML in 21 Days]

----

= Complexity (Big O) =

[http://pages.cs.wisc.edu/~vernon/cs367/notes/3.COMPLEXITY.html Complexity & Big-O Notation]
[http://stackoverflow.com/questions/2307283/what-does-olog-n-mean-exactly Good Explanation of O(log n) from Stackoverflow]

----

= Maths =

[http://oakroadsystems.com/math/loglaws.htm Logarithms]

= Data Structures & Algorithms =

A good alternative from [http://en.wikipedia.org/ Wikipedia] articles on data structures and algorithms is http://www.algolist.net/ which provides examples in Java & C++ along with explanations and analysis.

[http://so-i-think-i-created-my-first-online-project.googlecode.com/files/Data%20Structures%20and%20Algorithms%20in%20Java%20Fourth%20Edition.pdf?bcsi_scan_4c5c01dba4894524=0&bcsi_scan_filename=Data%20Structures%20and%20Algorithms%20in%20Java%20Fourth%20Edition.pdf Data Structures & Algorithms in Java (4th Edition)]

----

== Data Structures ==

=== Trees ===

==== Depth ====
*Formal Definition:* Let _v_ be a node of a tree _T_. The depth of _v_ is the number of ancestors of _v_, excluding _v_ itself. If _v_ is the root, then the depth of _v_ is 0.
*Informal Definition:* How deep down the tree is the node you're concerned with?

==== Height ====
*Formal Definition:* The height of a node _v_ in a tree _T_ is defined recursively:<br><li>If _v_ is an external node, then the height of _v_ is 0
<br><li>Otherwise, the height of _v_ is one plus the maximum height of a child of _v_.<br>
*Informal Definition:* So basically how far is _v_ away from it's further descendant which is a leaf (external node). 

Adapted from [http://en.wikipedia.org/wiki/Red_black_tree Red-Black Tree]: Both Red-Black tree ([http://www.ece.uc.edu/~franco/C321/html/RedBlack/redblack.html demo]) and AVL trees ([http://www.strille.net/works/media_technology_projects/avl-tree_2001/ demo]) support O(log n) search, insertion, and removal. AVLs are more rigidly balanced than red–black trees, leading to slower insertion and removal but faster retrieval. This makes AVLs attractive for data structures that may be built once and loaded without reconstruction, such as language dictionaries (or program dictionaries, such as the opcodes of an assembler or interpreter).

2-4 Trees ([http://www.cs.unm.edu/~rlpm/499/ttft.html demo]) are fundamental to understanding red-black trees because the insertion and deletion operations on 2-4 trees are also equivalent to colour-flipping and rotations in red–black trees, despite this 2-4 trees are seldom used in practice. Try walking through the [http://en.wikipedia.org/wiki/2-4_tree#Example example] from wikipedia of a 2-3-4 tree insertion.

*Differentiating Red-Black Trees*

Although AVL trees and (2,4) trees have a number of nice properties, there are some dictionary applications for which they are not well suited. For instance, AVL trees may require many restructure operations (rotations) to be performed after a removal, and (2,4) trees may require many fusing or split operations to be performed after either an insertion or removal. The red-black tree data structure does not have these drawbacks, however, as it requires that only O(1) structural changes be made after an update in order to stay balanced.

=== Heaps ===

The description from Wikipedia for the [http://en.wikipedia.org/wiki/Heap_%28data_structure%29 Heap] data structure is very clear on concise. Essentially, the structure is a binary tree where some form of ordering is used to determine where each node is placed, for instance if you were ordering percentages on a max-heap 100 would be at the top, 1 at the bottom. When you add a node to this type of structure you always fill the next level with empty places from the left to the right, once you add a value you compare it to it's parent and if ordering dictates that the new node should come before it's parent you swap them in the structure (and carry on until you don't need to swap or you reach the root). Similarly if you remove the root you then swap the most recently added item into the root's position and compare the new root with each of it's children, swapping with the most appropriate child e.g. the larger of the two in the case of a max-heap. Again you continue this process until you don't need to swap again or you reach a leaf.

Heaps are used in Heap Sorts to achieve worst case performance of O(n log n).

In Java the [http://docs.oracle.com/javase/6/docs/api/java/util/PriorityQueue.html Priority Queue] is an example of a min-heap.

=== Sets ===
A Set stores unique elements.

=== Maps ===
In the conventional sense of the word a Map is a data structure which maps a key to a value, each key should map to exactly one value.

==== LinkedHashMap ====
Hash table and linked list implementation of the Map interface, with predictable iteration order. This implementation differs from HashMap in that it maintains a doubly-linked list running through all of its entries. This linked list defines the iteration ordering, which is normally the order in which keys were inserted into the map (insertion-order). Note that insertion order is not affected if a key is re-inserted into the map. (A key k is reinserted into a map m if m.put(k, v) is invoked when m.containsKey(k) would return true immediately prior to the invocation.)

There is also an equiv. for Sets called LinkedHashSet, which could be compared to TreeSet and HashSet.

==== MultiMap ====
A multimap is a generalisation of a map or associative array abstract data type in which more than one values may be associated with and returned for a given key. Often multimap is implemented as a map wth lists or sets as the map values. Both Apache Commons Collections & Google Collections offer MultiMap implementations.

=== Stacks ===

=== Lists ===

=== Queues ===
A Queue is a basic Collection in Java that offers 5 additional methods for interacting with the collection in a queue fashion:
{{{
    boolean offer(E e); // inserts, if possible
    E poll();           // retrieves and removes head of queue
    E remove();         // like poll() but throws exception
    E peek();           // retrieves head without removing
    E element();        // like peek() but throws exception
}}}

==== Concurrent Linked Queue ====
 * Unbounded thread-safe Queue based on linked nodes
 * Elements ordered FIFO
 * Used when many threads share access to a queue
 * The size() method is not constant time (it have to count all elements each time)
 * can often be used as a thread-safe low-contention list

==== Priority Queue ====
 * Unbounded priority queue based on priority heap
 * Orders elements according to order specified at construction time (either natural order or via Comparable, or by Comparator)
 * Least element is at the head of the queue (this queue is not stable, thus when two elements are equals their insertion order is not maintained - you would have to provide this mechanism yourself)
 * A normal Priority Queue is not thread-safe, however a Priority Blocking Queue (see below) is.
 * If you need to see the items in the queue in reverse order (i.e. largest first) then just create a reverse comparator and use that for head building.

=== Block Queues ===

We also have the Blocking Queue interface which includes the `put()` and `take()` methods which block until space or elements become available (respectively). These types of queue are frequently used in Producer/Consumer scenarios.

There are various Blocking Queue implementations:
 * *ArrayBockingQueue* - underlying container is an array
 * *LinkedBlockingQueue* - Linked queues typically have higher throughput than array-based queues but less predictable performance in most concurrent applications.
 * *DelayQueue* - Only delivers after some time-out has expired
 * *PriorityBlockingQueue* - Returns by sort order, smallest elements first
 * *SynchronousQueue* - Queues of size zero, used if you do not want asynchronicity

=== Deques ===
A Deque is a double‐ended queue that allows efficient insertion and removal from both the head and the tail. Implementations include ArrayDeque and LinkedBlockingDeque.

Just as blocking queues lend themselves to the producer-consumer pattern, deques lend themselves to a related pattern called work stealing. A producer-consumer design has one shared work queue for all consumers; in a work stealing design, every consumer has its own deque. If a consumer exhausts the work in its own deque, it can steal work from the tail of someone else's deque. Work stealing can be more scalable than a traditional producer-consumer design because workers don't contend for a shared work queue; most of the time they access only their own deque, reducing contention. When a worker has to access another's queue, it does so from the tail rather than the head, further reducing contention.

Deques are introduced in section 5.3.3 of Java Concurrency in Practice.

=== LRU Cache ===


== Sorting ==

There are lots of different types of sorting algorithms, only some are covered here. A comparison of the space complexity of many of those featured can be found [http://en.wikipedia.org/wiki/Timsort#Performance here].

=== Stable vs. Unstable Sorts ===
*Stable* sorting algorithms maintain the relative order of records with equal keys.<br>
*Unstable* sorting algorithms may change the relative order of records with equal keys, but stable sorting algorithms never do so.<br>
Unstable sorting algorithms can be specially implemented to be stable. One way of doing this is to artificially extend the key comparison, so that comparisons between two objects with otherwise equal keys are decided using the order of the entries in the original data order as a tie-breaker. Remembering this order, however, often involves an additional computational cost.

=== Selection Sort - O(n^2^) ===
*Basic Idea*: Ascend the unsorted array until the find the smallest value in the list, swap that value with the value in the first spot, then search adjust the position you are attempting to fill and search again for the next smallest etc. continuing until you reach the end and everything is sorted.

Selection sort is sometimes useful for small data sets where memory is limited.
Worst Case Performance: O(n^2^)
Best Case Performance: O(n^2^)
Average Case Performance: O(n^2^)

=== Insertion Sort - O(n^2^) ===
*Basic Idea*: Insertion sort is a simple sorting algorithm that is relatively efficient for small lists and mostly sorted lists, and often is used as part of more sophisticated algorithms. It works by taking elements from the list one by one and inserting them in their correct position into a new sorted list. In arrays, the new list and the remaining elements can share the array's space, but insertion is expensive, requiring shifting all following elements over by one.

Generally speaking (and despite having same worst case complexity) an Insertion Sort is expected to perform better than a Selection Sort.

Worst Case Performance: O(n^2^) comparisons, swaps
Best Case Performance: O(n) comparisons, O(1) swaps
Average Case Performance: О(n^2^) comparisons, swaps

[http://en.wikipedia.org/wiki/Insertion_sort#Comparisons_to_other_sorting_algorithms Comparisons]

=== Bubble Sort - O(n^2^) ===
*Basic Idea*: Bubble sort works by repeatedly stepping through the list to be sorted, comparing each pair of adjacent items and swapping them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, which indicates that the list is sorted.

Worst Case Performance: O(n^2^)
Best Case Performance: O(n)
Average Case Performance: O(n^2^)

=== Quick Sort - O(n log n) (although not guaranteed) ===
*Basic Idea*: Is similar to merge sort in that is it a divide and conquer algorithm. [http://opendatastructures.org/versions/edition-0.1e/ods-java/11_1_Comparison_Based_Sorti.html#SECTION001412000000000000000 link] The basic idea is to pick a pivot value and move elements around (by swapping them) so that they fall on the correct side of the pivot value.

As the the Java Arrays API http://docs.oracle.com/javase/6/docs/api/java/util/Arrays.html#sort(int[]) Java uses a tuned quicksort to perform it's sorting, the algorithm offers n*log(n) performance on many data sets that cause other quicksorts to degrade to quadratic performance. That sort I have seen it suggested elsewhere that it uses Merge Sort

Worst Case Performance: O(n^2^) - although in practice this is very rare behaviour.
Best Case Performance: O(n log n)
Average Case Performance: O(n log n)
NOTE: Quick Sorts space complexity is O(log n) where as Merge Sorts is O(n)

=== Merge Sort - O(n log n) (guaranteed) ===
*Basic Idea*: This is a classic divide and conquer algorithm. Recursively halve your array until each item is on its own, then scan the first two items placing them in the correct order, complete this process with the next two items until each single item has been ordered as a pair (ignore odds), then take the first two pairs of sorted lists and scan both to create a order list of 4 items, keep going until the list of back together.

Worst Case Performance: O(n log n)
Best Case Performance: O(n log n)
Average Case Performance: O(n log n)

=== Timsort - O(n log n) ===
*Basic Idea*: Timsort is a hybrid sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. The algorithm finds subsets of the data that are already ordered, and uses the subsets to sort the data more efficiently. This is done by merging an identified subset, called a run, with existing runs until certain criteria are fulfilled.

Although it originated as a search for the Python programming language, as of Java SE 7 Timsort will be used to Array in Java as well.

Worst Case Performance: O(n log n)
Best Case Performance: O(n)
Average Case Performance: O(n log n)

=== Hash Sort - O(n) ===
*Basic Idea*: Maintain an array of buckets of items which is at least 20-25% larger than your list of items (to reduce clashes) and perform the same calculation on each item to return a numerical representation  of that item such that you can index into the array with that.

*Java Specialist Masters Course*
 * Traditionally (before Java 1.4), the HashMap would contain a sparse array holding pointers to the keys
 * The remainder of dividing hash code by the number of buckets in the sparse array would give the offset
 * When there was a clash, the equals() method was used to ascertain uniqueness (having lots of objects with the same hash code would result in a long list to iterate through)
 * It was important to have a prime number hash map size and to have fairly random distribution of hash codes

 * Since Java 1.4 the HashMap uses bit masking to put the keys into buckets (thus the size of HashMap is always a power of 2 [2,4,8,16,32,34,128,256,...] - if you size the HashMap differently (e.g. 101) it will automatically be resized to one of these values
 * A big disadvantage with bit masking is that it is far more sensitive to bit patterns than the remainder approach (which was ditched because it is quite an expensive operation)
 * The bits are then being mixed around with re-hashing

== Searching ==
=== Linear Search - O(n) ===
*Basic Idea*: Just search through the list from the beginning until you find the item you're looking for. With a linear search it tends* not to make much difference if the list is sorted or not (although you could have a very clever sort designed to place the most frequently used items at the front of the list).

Worst Case Performance: n
Best Case Performance: 1
Average Case Performance: n/2

=== Binary Search - O(log n) ===
*Basic Idea*: Particularly applicable to the tree structures discussed above. The tree must already be sorted and balanced if you want to achieve O(log n) guaranteed, remember that this take time and power in the first place.

Worst Case Performance: O(log n)
Best Case Performance: 1
Average Case Performance: ~O(log n)

=== Hash Search - O(1) ===
*Basic Idea*: Index into the list using the hash code you calculate for your item, if there are clashes you might not achieve your constant time target however if you stick to only ever being ~75-80% full then you shouldn't be too far off.

Hashes are not very efficient in terms of space because by definition you need some empty slots.

----

= Design Patterns =

----

= The OSI Application Model =

The [http://en.wikipedia.org/wiki/OSI_model Open Systems Interconnection] (OSI) model is a model used standardise the functions of a communications system in terms of abstraction layers. Similar communication functions are grouped into logical layers. A layer serves the layer above it and is served by the layer below it.

The 7 layers from top (nearest the user) to bottom are:<br>
*Application*<br>
*Presentation*<br>
*Session*<br>
*Transport*<br>
*Network*<br>
*Data Link*<br>
*Physical*

Below the are describe from bottom to top...

== Physical ==
The *Physical Layer* describes the physical properties of the various communications media; as well as the electrical properties and interpreation of the exchanged signals.<br/>
Example: this layer defines the size of the Ethernet coaxial cable, the type of the BNC connector used, and the termination method. e.g. IEEE 802.11

== Data Link ==
The *Data Link Layer* describes the logical organisation of data bits transmitted on a particular medium.<br/>
Example: this layer defines the framing, addressing and checksumming of Ethernet packets. e.g. Point-to-Point Protocol (PPP)

== Network ==
The *Network Layer* describes how a series of exchanges over various data links can deliver data between any two modes in a network.<br>
Example: this layer defines the addressing and routing structure of the Internet. e.g. IP (v4/v6)

== Transport ==
The *Transport Layer* describes the quality and nature of the data delivery.<br>
Example: this layer defines if and how retransmissions will be used to ensure data delivery. e.g. TCP/UDP

== Session ==
The *Session Layer* describes the organisation of data sequences larger than the packets handled by lower layers.<br>
Example: the layer describes how request and reply packets are paired in a remote procedure call. e.g. TSL/SSL

== Presentation ==
The *Presentation Layer* describes the syntax of data being transferred. <br>
Example: this layer describes how floating point numbers can be exchanged between hosts with different math formats. e.g. MIME

== Application ==
The *Application Layer* describes how real work actually gets done.<br>
Example: this layer would implement file system operations. e.g. HTTP, DHCP, NFS

----

= Architectures (32-bit vs. 64-bit) =

A 64-bit computer architecture generally has integer and addressing registers that are 64 bits wide, allowing direct support for 64-bit data types and addresses.

== 64-bit Java ==

For Java specific 64-bit information see the [http://www.oracle.com/technetwork/java/hotspotfaq-138619.html#64bit_description FAQ]

A compiled Java program can run on a 32- or 64-bit Java virtual machine without modification. The lengths and precision of all the built-in types are specified by the standard and are not dependent on the underlying architecture. Java programs that run on a 64-bit Java virtual machine have access to a larger address space (this is the primary advantage), this allows for a much larger Java heap size and an increased maximum number of Java Threads, which is needed for certain kinds of large or long-running applications.

The additional overhead of having native pointers in the system which take up 8 bytes instead of 4 means that a small performance penalty is often observed when porting from 32-bit to 64-bit Java, however the the benefits of being able to address larger amounts of memory might make that it worth it. Any degradation will depend on the amount of pointer accessing your application performs.

=== IMPORTANT ===

*What it is NOT*:  Many Java users and developers assume that a 64-bit implementation means that many of the built-in Java types are doubled in size from 32 to 64.  This is not true.  We did not increase the size of Java integers from 32 to 64 and since Java longs were already 64 bits wide, they didn't need updating.  Array indexes, which are defined in the Java Virtual Machine Specification, are not widened from 32 to 64.  We were extremely careful during the creation of the first 64-bit Java port to insure Java binary and API compatibility so all existing 100% pure Java programs would continue running just as they do under a 32-bit VM.